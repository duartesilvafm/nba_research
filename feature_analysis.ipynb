{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze what are the most important indicators whether a team will win or lose, we will now build a simple binary predictor model and study which are the variables which help predict wins and losses more.\n",
    "\n",
    "Since we are only trying to analyze the features, we will use the same game variables for our x values, since the aim is not build predictions.\n",
    "\n",
    "To analyze the importance of the features, we will use the following models:\n",
    "\n",
    "* Logistics Regression\n",
    "* Random Forest Classifier\n",
    "* Gradient Boost Classifier\n",
    "* XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\n",
    "import joblib\n",
    "\n",
    "from modules.model import Model, ML_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data = pd.read_csv(f'./data/team_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, this will be the following features we will include:\n",
    "\n",
    "* fg3a\n",
    "* fg2a\n",
    "* fta\n",
    "* fg3_pct\n",
    "* fg2_pct\n",
    "* ft_pct\n",
    "* ast_ratio\n",
    "* team_tov_pct\n",
    "* team_orb_pct\n",
    "* team_efg_pct\n",
    "\n",
    "Our target variable will be the Win variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = ['fg3a', 'fg2a', 'fta', 'fg3_pct', 'fg2_pct', 'ft_pct', 'ast_ratio', 'team_tov_pct', 'team_orb_pct', 'team_efg_pct']\n",
    "y_var = ['win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fg3a</th>\n",
       "      <th>fg2a</th>\n",
       "      <th>fta</th>\n",
       "      <th>fg3_pct</th>\n",
       "      <th>fg2_pct</th>\n",
       "      <th>ft_pct</th>\n",
       "      <th>ast_ratio</th>\n",
       "      <th>team_tov_pct</th>\n",
       "      <th>team_orb_pct</th>\n",
       "      <th>team_efg_pct</th>\n",
       "      <th>win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>52</td>\n",
       "      <td>21</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>15.595758</td>\n",
       "      <td>12.9</td>\n",
       "      <td>20.9</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "      <td>24</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>18.524236</td>\n",
       "      <td>13.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>18.773467</td>\n",
       "      <td>14.6</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>63</td>\n",
       "      <td>38</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>14.490927</td>\n",
       "      <td>11.8</td>\n",
       "      <td>30.2</td>\n",
       "      <td>0.476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>14.713408</td>\n",
       "      <td>9.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fg3a  fg2a  fta   fg3_pct   fg2_pct    ft_pct  ast_ratio  team_tov_pct  \\\n",
       "0    33    52   21  0.393939  0.461538  0.714286  15.595758          12.9   \n",
       "1    31    50   24  0.354839  0.620000  0.708333  18.524236          13.3   \n",
       "2    45    57   20  0.422222  0.421053  0.850000  18.773467          14.6   \n",
       "3    40    63   38  0.350000  0.444444  0.842105  14.490927          11.8   \n",
       "4    43    65   28  0.302326  0.507692  0.785714  14.713408           9.8   \n",
       "\n",
       "   team_orb_pct  team_efg_pct  win  \n",
       "0          20.9         0.512    0  \n",
       "1          25.6         0.586    1  \n",
       "2          28.1         0.515    0  \n",
       "3          30.2         0.476    1  \n",
       "4          24.0         0.486    1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = team_data.copy(deep=True)\n",
    "model_data.columns = [x.lower().strip() for x in model_data.columns]\n",
    "model_data = model_data[x_vars + y_var]\n",
    "model_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the variables all have different ranges to them, we will min max scale all variables above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vars for train test split\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "SEED = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = model_data[x_vars].values, model_data[y_var].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scale variables\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "X_scaled = min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Analysis with tree based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and the parameter grid for which to search\n",
    "rf = {\"name\": \"rf\", \"classifier\": RandomForestClassifier(), 'searcher':GridSearchCV\\\n",
    "    , \"param_grid\": {\"max_depth\":[6,7,8,9,10], \"n_estimators\":[150,200,250,300,400]}, \\\n",
    "        'max_features': [4,5,6,7]}\n",
    "\n",
    "gb = {\"name\": \"gb\", \"classifier\": GradientBoostingClassifier(), 'searcher':RandomizedSearchCV\\\n",
    "    , \"param_distributions\": {\"max_depth\":[6,7,8,9,10], \"n_estimators\":[150,200,250,300,400], \\\n",
    "        \"learning_rate\": [0.1, 0.05, 0.01, 0.001], 'max_features': [4,5,6,7], \\\n",
    "            'subsample': [0.5, 0.6, 0.7, 0.8, 0.9]}}\n",
    "\n",
    "xgb = {\"name\": \"xgb\", \"classifier\": XGBClassifier(verbosity = 0), 'searcher':RandomizedSearchCV\\\n",
    "    , \"param_distributions\": {\"max_depth\":[6,7,8,9], \"n_estimators\":[150,200,250,300], \\\n",
    "        \"eta\": [0.1, 0.05, 0.01, 0.001], 'subsample': [0.5, 0.6, 0.7, 0.8, 0.9]}}\n",
    "\n",
    "algorithms_params = [gb, xgb, rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cross validation searchers\n",
    "gridsearch, randomsearch = GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_commands(model, searcher: object, searcher_params: dict, \\\n",
    "    metrics: list=[\"accuracy\"], test_size: float=0.2):\n",
    "    \n",
    "    \"\"\"\n",
    "    function to run the commands of a model object\n",
    "    \"\"\"\n",
    "    model.get_best_params(searcher=searcher, searcher_params=searcher_params, \\\n",
    "        metrics=metrics)\n",
    "    model.set_params(params_to_set=model.best_params)\n",
    "    model.train_test_split(test_size=test_size)\n",
    "    model.train_model()\n",
    "    model.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running commands GradientBoostingClassifier() & accuracy...\n",
      "searching for best parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SILVAFRA\\Anaconda3\\envs\\nba\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7351120535626494\n",
      "With the following parameters: {'subsample': 0.5, 'n_estimators': 150, 'max_features': 6, 'max_depth': 7, 'learning_rate': 0.01}\n",
      "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.01, 'loss': 'log_loss', 'max_depth': 7, 'max_features': 6, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 150, 'n_iter_no_change': None, 'random_state': None, 'subsample': 0.5, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SILVAFRA\\Anaconda3\\envs\\nba\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:437: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model train score: 0.8268518518518518\n",
      "Model test score: 0.7362962962962963\n",
      "storing model ...\n",
      "running commands XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...) & accuracy...\n",
      "searching for best parameters\n",
      "Best score: 0.7358528382641724\n",
      "With the following parameters: {'subsample': 0.7, 'n_estimators': 200, 'max_depth': 6, 'eta': 0.01}\n",
      "{'objective': 'binary:logistic', 'use_label_encoder': None, 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 6, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 200, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.7, 'tree_method': None, 'validate_parameters': None, 'verbosity': 0, 'eta': 0.01}\n",
      "Model train score: 0.7985185185185185\n",
      "Model test score: 0.7459259259259259\n",
      "storing model ...\n",
      "running commands RandomForestClassifier() & accuracy...\n",
      "searching for best parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SILVAFRA\\Anaconda3\\envs\\nba\\lib\\site-packages\\sklearn\\model_selection\\_search.py:910: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7355573320653899\n",
      "With the following parameters: {'max_depth': 7, 'n_estimators': 400}\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 400, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SILVAFRA\\OneDrive - Anheuser-Busch InBev\\My Documents\\03_Data_Science\\Training\\nba_research\\modules\\model.py:39: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.base_model.fit(self.X_train, self.y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model train score: 0.7927777777777778\n",
      "Model test score: 0.7348148148148148\n",
      "storing model ...\n"
     ]
    }
   ],
   "source": [
    "estimators = {}\n",
    "\n",
    "# run through the different algorithms chosen\n",
    "for algorithm in algorithms_params:\n",
    "\n",
    "    for metric in [\"accuracy\"]:\n",
    "\n",
    "        # create a deep copy of the object\n",
    "        algorithm_copy = copy.deepcopy(algorithm)\n",
    "\n",
    "        # create the object for the model\n",
    "        model = ML_Model(X=X_scaled, y=y, base_model=algorithm_copy[\"classifier\"], \\\n",
    "            seed=SEED)\n",
    "    \n",
    "        # define searcher and searcher params\n",
    "        searcher=algorithm_copy['searcher']\n",
    "        params={'estimator': model.base_model, 'scoring':metric, 'n_jobs':-1}\n",
    "\n",
    "        # add the param distribution based on the searcher\n",
    "        if searcher==GridSearchCV:\n",
    "            params['param_grid'] = algorithm_copy['param_grid']\n",
    "        else:\n",
    "            params['param_distributions'] = algorithm_copy['param_distributions']\n",
    "            params['n_iter'] = 15\n",
    "\n",
    "        # run the commands from the class necessary to create the model\n",
    "        print(f\"running commands {algorithm_copy['classifier']} & {metric}...\")\n",
    "        run_model_commands(model=model, searcher=searcher, searcher_params=params, \\\n",
    "            metrics=[metric])\n",
    "\n",
    "        print(\"storing model ...\")\n",
    "\n",
    "        # storing the file\n",
    "        name = algorithm_copy['name']\n",
    "        file_object = Path(f\"models/{name}_model.pkl\").open(\"wb\")\n",
    "        joblib.dump(model.base_model, file_object)\n",
    "\n",
    "        # saving it in the dictionary\n",
    "        estimators[f\"{algorithm_copy['name']}_{metric}\"] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "655ed73ecbb98cb9b76947a7ee1a2f2570c49d19864a12727ffd90473a067d71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
